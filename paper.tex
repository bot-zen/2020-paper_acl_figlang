%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%%% by estemle
\usepackage[utf8x]{inputenc}
\usepackage[autostyle=none]{csquotes}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{siunitx}
\sisetup{input-ignore={,},group-separator={,},input-decimal-markers={.}}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}


%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Testing the role of metadata in a neural network architecture of metaphor identification}

\author{Egon W.~Stemle \\
  Eurac Research / Bolzano-Bozen (IT) \\
  Masaryk University / Brno (CZ) \\
  {\tt egon.stemle@eurac.edu} \\\And
  Alexander Onysko \\
  University of  Klagenfurt / Klagenfurt (AT) \\
  {\tt alexander.onysko@aau.at} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper describes the application of a neural network for the automatic detection of metaphors. The system participated in the shared task of metaphor identification that is part of the Second Workshop of Figurative Language Processing held at ACL 2020. The particular focus of our approach is on the potential influence that the metadata given in the ETS Corpus of Non-Native Written English might have on the automatic detection of metaphors in this dataset. The article first discusses the annotated ETS learner data, highlighting some of its peculiarities and inherent biases of metaphor use. This is followed by a series of evaluations performed by an LSTM BiRNN classification architecture in order to test whether specific metadata have an influence on the system performance in this task of automated metaphor identification. 
\end{abstract}



\section{Introduction}

Research on metaphors, particularly in the framework of conceptual metaphor theory, continues to grow in all genres of language use and across diverse disciplines of linguistics (cf., among others, \citet{littlemore:2019:MetaphorsMindSources}; \citet{gibbsjr:2017:MetaphorWarsConceptual}; \citet{charteris-black:2016:FireMetaphorsDiscourses}; \citet{kovecses:2020:ExtendedConceptualMetaphor}; \citet{callies-degani::MetaphorLanguageCulture} for recent and forthcoming overviews and extensions, and \citet{veale-EtAl:2016:MetaphorComputationalPerspective} for a book-length discussion of computational linguistic perspectives). While the importance of metaphor in thought and everyday language use has long been acknowledged \citep{LakoffJohnson80}, the practice of metaphor research still faces two methodological and analytical challenges: first of all, the identification of metaphors and, secondly, their actual description through source and target domains. 

In computational linguistics, a great amount of recent work has been concerned with addressing the challenge of identifying metaphors in texts. This is evident in the series of four Workshops on Metaphor in NLP from 2013 to 2016 and in the two Workshops on Figurative Language Processing in 2018 and 2020, each of which involved a shared task in automatic metaphor detection. Identification systems that achieved the best results in the first shared task relied on neural networks incorporating LSTM architectures (see \citealp{mu-EtAl:2019} for a discussion). 
This paper extends a system proposed in \citet{stemle-onysko:2018:naacl-flpst}, which uses word-embeddings combining corpora like the BNC with the TOEFL11 language learner corpus (see \citealp{Blanchard-EtAl:TOEFL11}). With this system, we participate in the \emph{The Second Shared Task (ST) on Metaphor Detection}. The difference to the 2018 edition of the ST is a new set of data. As in the first task, one part of the dataset is based on the VU Amsterdam (VUA) Metaphor Corpus manually annotated according to the MIPVU procedure \citep{Steen2010}. Additionally, the second task includes a sample of 240 argumentative learner texts. These texts are taken from the ETS Corpus of Non-Native Written English (synonymous to the TOEFL11 corpus) and manually annotated \citep{beigmanklebanov-EtAl:2018}. 

Since the learner essays are a very specific kind of data, the aim of this study is to build upon observations from \citet{stemle-onysko:2018:naacl-flpst} and further investigate factors in the data that can influence the results of neural network based metaphor identification that uses word-embeddings derived from the larger set of the same data type (the TOEFL11 corpus). For that, a particular focus in the training and testing is put on the metadata, specifically on proficiency, prompt and L1. 

To address these aims, our paper is structured as follows: Section 2 provides some observations on the annotated learner corpus dataset. Section 3 describes the system of metaphor identification. This is followed in Section 4 by the results of the experiments, which are briefly discussed in light of the observations on the annotated learner corpus data. 

\section{Observations on the data}

The VUA Metaphor Corpus and its application in the first shared task has been concisely described in \citet{leong-EtAl:2018:PFLP}. The authors have reported the relatively high inter-annotator agreement ($\kappa>0.8$), which is in part due to the MIPVU protocol \citep{Steen2010} and the close training of annotators in the Amsterdam Metaphor Group. Interestingly, the results of the first task across all submitted systems showed a clear genre bias with academic texts consistently displaying the highest correct identification rates and conversation data (i.e.~spoken texts) the lowest \citet[p.60]{leong-EtAl:2018:PFLP}. This might be related to the fact that academic discourse is more schematic and formulaic (e.g.~in the use sentential adverbials and verbal constructions) and might rely to a greater extent on recurrent metaphorical expressions than spoken conversations, which are less schematic and can thus display a higher degree of syntactic and lexical variation. In other words, similarities in the data between training and test sets might be higher in the academic than in the conversation genre, leading to different genre-specific training effects in neural networks.  

Apart from the VUA metaphor corpus, the second shared task introduces a novel dataset culled from the ETS Corpus of Non-Native Written English. In their description, \citet{beigmanklebanov-EtAl:2018} report an average inter-annotator agreement of $\kappa=0.62$ on marking argumentation-relevant metaphors. Disagreement in the annotations was positively compensated in that all metaphor annotations were included even if only given by one of the two raters. While a focus on argumentation-relevant metaphors coheres with the genre of short argumentative learner essays written in response to one of eight prompts during TOEFL examinations in 2006-2007 \citep{ETS2:ETS202331}, the scope of metaphor annotation is more restricted in the ETS sample than in the VUA corpus, which follows the more stringent MIPVU protocol. This explains to some extent why the overall amount of metaphor-related words in the training sets is considerably lower in the ETS sample (an average of 7\% in All-POS and 14\% among verbs; see \citealp[p.88]{beigmanklebanov-EtAl:2018}) than in the VUA Metaphor Corpus (15\% in All-POS and 28.3\% among verbs; see Leong et al., 2018: 58). 

The relatively small size of the ETS sample inspired us to look into the structure of the data more closely to check whether any potential biases exist that might play a role for the automatic detection of metaphors. \citet[p.89]{beigmanklebanov-EtAl:2018} report a significant positive correlation of the number of metaphors and the proficiency ratings of the texts as medium or high in the data. This relationship is confirmed by an independent samples t-test in the training partition of the data (180 texts). The group of highly proficient learners ($N=95$) uses more metaphors ($M=13.98, SD=8.23$) compared to the group of medium proficient learners ($N=85, M=9.55, SD=5.96$) at a significantly higher rate ($t=4.07, p=0.000071$). The L1 background of the learners (i.e. Arabic, Italian, Japanese) did not influence the mean number of metaphors in the texts as confirmed by a one-way ANOVA ($F=1.619, p=0.201$; L1 Arabic: $N=63, M=12.48, SD=8.37$; L1 Italian: $N=59, M=12.71, SD=7.79$; L1 Japanese: $N=59, M=10.44, SD=6.38$). 

\begin{table}[t]
\begin{center}
%\small
\begin{tabular}{|l|c|c|c|c|c|H c|}
\hline
    \multicolumn{1}{ |c|}{\rotatebox{90}{\textbf{Prompt}            }}		& 
    \multicolumn{1}{ c| }{\rotatebox{90}{\# words                   }}		& 
    \multicolumn{1}{ c| }{\rotatebox{90}{\# metaph. types              }}		& 
    \multicolumn{1}{ c| }{\rotatebox{90}{\# metaph. tokens             }}		& 
    \multicolumn{1}{ c| }{\rotatebox{90}{Metaph. type/toks             }} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{\% metaph. per words        }} 		&
    \multicolumn{1}{ H  }{\rotatebox{90}{rate of Metaph. reoccurrence\    }} 		&
    \multicolumn{1}{ c| }{\rotatebox{90}{mean \# of metaph.}} 		\\
\hline
\hline
\textbf{P1}  & 8059  & 205       & 361   & 0.57  & 4.5     & 22.3  & 15.696 \\
\hline

\textbf{P2}  & 7493  & 199       & 330   & 0.60  & 4.4     & 22.7  & 15.714 \\
\hline

\textbf{P3}  & 7947  & 222       & 397   & 0.59  & 4.8     & 21.0  & 17.227 \\
\hline

\textbf{P4}  & 8076  & 146       & 173   & 0.84  & 2.1     & 46.7  & 7.522 \\
\hline

\textbf{P5}  & 8455  & 172       & 206   & 0.83  & 2.4     & 41.0  & 8.957 \\
\hline

\textbf{P6}  & 8446  & 134       & 188   & 0.71  & 2.2     & 44.9  & 7.833 \\
\hline

\textbf{P7}  & 7516  & 170       & 243   & 0.70  & 3.2     & 30.9  & 11.045 \\
\hline

\textbf{P8}  & 7923  & 197       & 260   & 0.76  & 3.3     & 30.4  & 11.818 \\

\hline
\end{tabular}
\end{center}
\caption{\label{tab:num_metaphors}Number of metaphors (types and tokens) per prompt in the annotated ETS training set} 
\end{table}

Since the Corpus of Non-Native Written English consists of argumentative learner essays that were written in response to one of eight different prompts, another factor to consider in the annotated ETS sample is the role the prompt might have on metaphor use. Table \ref{tab:num_metaphors} summarizes the descriptive statistics on the number of metaphors per prompt.  


From left to right, the columns in Table \ref{tab:num_metaphors} report, per prompt, the total number of words, the number of metaphor types, the overall number of metaphor tokens (i.e. all words annotated for metaphor), the type token ratio of metaphors, the relative amount of metaphor tokens among all words, and the mean values of metaphors. The two rightmost columns illustrate an uneven occurrence of metaphors across the diverse prompts. Three groups emerge from the data according to their similarly high (or low) values: P1, P2, P3 as the highest scoring group, P4, P5, P6 as the lowest scoring group, and P7, P8 whose values are in-between the other two groups. A one-way ANOVA for independent samples ($F=7.0919, p=.00001$) confirms a significant difference between the groups. T-tests comparing the minimal and maximal numerical distances between the high, the medium, and the low clusters show that the high cluster is significantly different from the low cluster (P1: $N=23, M=15.70, SD=8.138$ compared to P5: $N=23, M=8.96, SD=4.117$) at $t(44)=3.54, p=.000948$. The differences between the low and the medium groups as well as the high and the medium group do not reach a significance threshold of $p<.01$.  

When looking for an explanation of these biased metaphor occurrences, some interesting patterns emerge among the high frequency group (P1, P2, and P3). In all these instances, the prompts trigger certain metaphor-related words (MRW) that occur at a high rate. Table 2 provides an overview of the metaphorical expressions triggered by the prompts P1, P2, and P3. The 30 most frequent MRW were closely analyzed for each of the prompts. 


\begin{table*}[t]
\setlength{\tabcolsep}{3pt}
\begin{center}
%\small
\begin{tabularx}{\textwidth}{|X|r|X|r|X|r|}
\hline
    \multicolumn{6}{|>{\hsize=\dimexpr8\hsize+14\tabcolsep+2\arrayrulewidth\relax}X|}{
        \textbf{P1:} It is better to have broad knowledge of many academic subjects than to specialize  in one specific subject.
    }
    \\
    \multicolumn{6}{|X|}{}
    \\
    \multicolumn{6}{|>{\hsize=\dimexpr8\hsize+14\tabcolsep+2\arrayrulewidth\relax}X|}{
        \textbf{P2:} Young people enjoy life more than older people do.
    }
    \\
    \multicolumn{6}{|X|}{}
    \\
    \multicolumn{6}{|>{\hsize=\dimexpr8\hsize+14\tabcolsep+2\arrayrulewidth\relax}X|}{
        \textbf{P3:} Young people nowadays do not give enough time to helping their communities.
    }
    \\
\hline
    \multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{\shortstack[l]{\textbf{metaphorical} \\ \textbf{expression}}}	
    &
    \shortstack[l]{\textbf{\# of} \\ \textbf{occur.}} 
    & 
    \multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{\shortstack[l]{\textbf{metaphorical} \\ \textbf{expression}}}
    &
    \shortstack[l]{\textbf{\# of} \\ \textbf{occur.}}
    &
    \multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{\shortstack[l]{\textbf{metaphorical} \\ \textbf{expression}}}
    &
    \shortstack[l]{\textbf{\# of} \\ \textbf{occur.}} 
    \\
\hline
\hline
    \multicolumn{2}{|l|}{\textbf{P1}}
    & 
    \multicolumn{2}{|l|}{\textbf{P2}}
    & 
    \multicolumn{2}{|l|}{\textbf{P3}}
    \\
\hline
%%
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
\enquote{broad(er) knowledge}
} &
60 &
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
have/-ing/has \newline
\enquote{have time}
} & 
55 & 
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
give/-s/-ing/-en \newline 
\enquote{give time}, \enquote{give help}
} & 
29
\\
%
\hline
%
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
focus/-ed \newline
\enquote{focus on a particular subject/field} 
} &
12 &
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
spend/-s/-ing \newline
\enquote{spend time/hours/years} } & 
14 & 
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
have/-ing/has \newline
\enquote{have time}
} & 
24
\\
%
\hline
%
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
*give/-s/-ing 
}		&
10 &
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
*face \newline
\enquote{face problems / change / responsibilities} 
} & 
6 & 
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
spend/-ing/-t  \newline
\enquote{spend time}
} & 
16
\\
%
\hline
%
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
wide/-er 
}		&
9 &
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
*get 
} & 
5 & 
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
support/-s/-ing/-ed  \newline
\enquote{support communities}
} & 
14
\\
%
\hline
%
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
*lead/-s 
} &
7 &
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
stage \newline
\enquote{stage of life} 
} & 
4 & 
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
*strong/-er/-ly
} & 
14 
\\
%
\hline
%
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
*spend 
}		&
6 &
 & 
 & 
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
free \newline
\enquote{free time}
} & 
9
\\
%
\hline
%
 &
 &
 & 
 & 
\multicolumn{1}{ |>{\hsize=\dimexpr2\hsize+4\tabcolsep+\arrayrulewidth\relax}X|}{
dedicate/-s \newline
\enquote{dedicate time}
} & 
9
\\
%%
\hline
\end{tabularx}
\end{center}
\caption{\label{tab:frequent_metaphors}Most frequent metaphorical expressions in P1, P2, and P3 \\ 
(*) MRW does not occur verbatim in the prompt }
\end{table*}


In Table \ref{tab:frequent_metaphors}, MRW that cannot be related to the prompt are preceded by an asterisk. All the other terms are triggered by the prompts. For P1, the expression \enquote{broad knowledge} from the prompt that instantiates an objectification metaphor of knowledge (\textsc{knowledge is an object}) is frequently reiterated in the test takersâ€™ essays and is by far the most frequent metaphorical expression among all annotated MRW in P1. The metaphorical uses of the lexeme \emph{focus} as in \enquote{focus on a particular subject/field} is triggered by the prompt as a synonymous phrase for \enquote{\ldots specialize in one specific subject}. Similarly, the term \emph{wide/-er} is used by some learners as a synonym of \enquote{broad knowledge}. In P2, the metaphorical phrase \enquote{have time} is prevalent. It is thematically triggered by the phrase \enquote{enjoy life}, which stimulates people to write about time as a (precious) possession that allows you to enjoy life. The metaphorical expression of \enquote{spending time} is evoked by the same conceptual metaphor. The \textsc{life is a journey} metaphor triggered by P2 is instantiated in the recurrent expression of stages in life. The mention of \enquote{time} in P3 evokes the same \textsc{time is a precious possession} conceptual metaphor as in P2. Again, the by far most recurrent MRW are the verbs \emph{give, have}, and \emph{spend} that objectify time in that metaphor. In addition, the use of the verb \emph{support} as in \enquote{support communities} is directly related to the prompt (\enquote{\ldots to helping their communities}) as are the metaphorical collocations \enquote{free time} and \enquote{dedicate time}. 

In all the other prompts, trigger effects do not occur or are not as quantitatively relevant as in P1 to P3. P4, for example, does not show any spikes in metaphor frequencies with the most frequent MRW (\emph{image}) merely occurring 5 times. The same is true in P5 with the terms \emph{ruining, reach, comfortable}, and \emph{advancement} being mentioned 4 times each as the most frequent MRWs. A weak effect can be observed in P6 where the prompt \enquote{The best way to travel is in a group led by a tour guide} triggers the metaphorical collocation to \enquote{take a trip} that recurs 14 times across the learner texts. In P7 and P8, the most frequent MRW are not stimulated by the prompt, and there are similarly low token frequencies leading to a flat frequency distribution among the MRW. Incidentally, P8 (\enquote{Successful people try new things and take risks rather than only doing what they already know how to do well}) contains the metaphorical expression \enquote{take risks} that recurs in the learner essays 43 times. However, it has not been manually annotated as an MRW.

If we take a look at how the MRW are distributed across the different parts of speech, it is interesting to note that verbs are by far more often marked as metaphors than nouns and adjectives/adverbs. Among the 30 most frequent MRW per prompt in the training set, 416 metaphorical verbs precede over 198 adjectives/adverbs and 141 nouns.

Finally, the learner data poses another peculiarity that is worth considering in the automatic metaphor identification. There is a total of 99 misspelled MRW across all prompts in the data (4.6\% of all MRW) as in \emph{messege, actractivity, strenght, knowled, isolte, dangerousness}, and \emph{broadn} to randomly pick out a few. Finding a way to factor in the out-of-vocabulary words will increase the performance of automatic metaphor detection. 

  
\section{Design and implementation}

Our approach extends the system from Stemle and Onysko (2018), which combines a small set of established machine learning (ML) methods and tools. In particular, the system uses fastText\footnote{\url{https://fasttext.cc/}} word embeddings from different corpora in a bi-directional recursive neural network architecture with long-term short-term memory (LSTM BiRNN) and implements a flat sequence-to-sequence neural network with one hidden layer using TensorFlow and Keras\footnote{\url{https://www.tensorflow.org/guide/keras}} in Python.
The experimental design uses word embeddings from corpora such as Wikipedia\footnote{https://fasttext.cc/docs/en/english-vectors.html} and BNC\footnote{https://embeddings.sketchengine.co.uk/static/index.html} as well as from texts of language learners (TOEFL11, Blanchard et al. 2013). This is to follow the intuition that the use of metaphors can vary depending on the competence of the learners and that these differences can be helpful in training a metaphor detection system.

For the current ST, the system was slightly extended: 
\begin{enumerate}
\item bump requirements of used tools to current versions, in particular: TensorFlow  + Keras, gensim\footnote{\url{https://radimrehurek.com/gensim/}}, FastText, and scikit-learn\footnote{\url{https://scikit-learn.org/}}
\item deal with the new format of the TOEFL data
\item make better use of sub-word information encoded in FastText word representations
\item add option to integrate metadata into the input representations for the neural network
\end{enumerate}


\section{Experiments}

\subsection{Combining VUA and TOEFL}
In a first experiment, we tried to extend the training data and combine the two datasets. 
Given the discussion in Section 2, we expected confounding effects due to the fact that the manual classification of All-POS metaphors is different in these two sets. 

As word embeddings, we used BNC and the complete TOEFL11 data.
The input data was constructed for both tasks (AllPOS and Verbs) by concatenating the test sets in both permutations. That is, first the TOEFL test set and then the VUA test set, and vice versa.
Testing was done by 10-fold cross-validation, which only approximates the proper sequence testing of input data, but we believe this will provide us with some valuable information.

Table \ref{tab:sequence_learning} shows that, most notably, the highest recall is achieved on the Verbs task when using first the TOEFL data and subsequently the VUA data. We interpret that in a way that the learning of the TOEFL data is mostly 'forgotten' by the neural network, but its focus on verb-metaphors leaves a strong initialization bias towards verbs. 

Overall, the results of the various runs show how different the two datasets are and, conversely, how much could be gained from homogeneously annotated data. 

Finally, we hope to be able to evaluate this experiment on the held-out test set from the organizers.

% \begin{table*}[t]
% \begin{center}
% \begin{tabular}{lll|l|l||l|l|}
% \cline{4-7}
% \multicolumn{3}{l}{}                                          &  \multicolumn{4}{|c|}{Task}  \\ 
% \cline{1-2} \cline{4-7} 
% \multicolumn{2}{|c|}{Training}  &  & \multicolumn{2}{|c||}{Verbs} & \multicolumn{2}{c|}{AllPOS} \\ 
% \hline 
% \multicolumn{2}{|c|}{In order 1:VUA 2:TOEFL} &    & VUA          & TOEFL         & VUA               & TOEFL       \\ \hline \hline
% \multicolumn{2}{|l|}{}      & Pr    & 0.48 (+/- 0.10) & 0.48 (+/- 0.08) & 0.47 (+/- 0.10)   & 0.47 (+/- 0.11) \\ \cline{3-7} 
% \multicolumn{2}{|l|}{}      & Re    & 0.49 (+/- 0.15) & 0.49 (+/- 0.14) & 0.41 (+/- 0.10)   & 0.41 (+/- 0.11) \\ \cline{3-7} 
% \multicolumn{2}{|l|}{}      & F1    & 0.48            & 0.48            & 0.44              & 0.44            \\ \hline \hline
% \multicolumn{2}{|c|}{In order 1:TOEFL 2:VUA} &  \multicolumn{5}{l}{}  \\ \hline \hline 
% \multicolumn{2}{|l|}{}      & Pr    &  0.39 (+/- 0.15) & 0.40 (+/- 0.15) & 0.32 (+/- 0.11) & 0.39 (+/- 0.23) \\ \cline{3-7}
% \multicolumn{2}{|l|}{}      & Re    &  0.79 (+/- 0.04) & 0.77 (+/- 0.05) & 0.71 (+/- 0.24) & 0.71 (+/- 0.24) \\ \cline{3-7} 
% \multicolumn{2}{|l|}{}      & F1    &  0.52            & 0.53            & 0.44            & 0.50            \\ \hline
% \multicolumn{2}{|c|}{Shuffled: TOEFL + VUA} &  \multicolumn{5}{l}{}  \\ \hline \hline 
% \multicolumn{2}{|l|}{}      & Pr    & 0.51 (+/- 0.02)  & 0.53 (+/- 0.03) & 0.52 (+/- 0.04) & 0.51 (+/- 0.03) \\ \cline{3-7}
% \multicolumn{2}{|l|}{}      & Re    & 0.69 (+/- 0.02)  & 0.67 (+/- 0.06) & 0.66 (+/- 0.06) & 0.67 (+/- 0.04) \\ \cline{3-7} 
% \multicolumn{2}{|l|}{}      & F1    & 0.59             & 0.59            & 0.58            & 0.58            \\ \hline
% \end{tabular}
% \end{center}
% \caption{\label{tab:sequence_learning}10-fold CV comparison of training on (un)shuffled VUA and TOEFL data for the Verb and AllPOS Task.}
% \end{table*}


\begin{table}[]
\setlength{\tabcolsep}{4.5pt}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{Training}               &    & \multicolumn{2}{c|}{Task}         \\ \hline \hline 
\multicolumn{2}{|c|}{\shortstack[l]{In order \\ 1:VUA \\ 2:TOEFL}} &    & Verbs           & AllPOS             \\ \hline
\multicolumn{2}{|l|}{}                       & Pr & 0.48 (+/- 0.10) & 0.47 (+/- 0.11) \\ \cline{3-5} 
\multicolumn{2}{|l|}{}                       & Re & 0.49 (+/- 0.15) & 0.41 (+/- 0.11) \\ \cline{3-5} 
\multicolumn{2}{|l|}{}                       & F1 & 0.48            & 0.44            \\ \hline \hline
\multicolumn{2}{|c|}{\shortstack[l]{In order \\ 1:TOEFL \\ 2:VUA}} & \multicolumn{3}{l}{}  \\ \hline
\multicolumn{2}{|l|}{}                       & Pr & 0.39 (+/- 0.15) & 0.39 (+/- 0.23) \\ \cline{3-5} 
\multicolumn{2}{|l|}{}                       & Re & 0.79 (+/- 0.04) & 0.71 (+/- 0.24) \\ \cline{3-5} 
\multicolumn{2}{|l|}{}                       & F1 & 0.52            & 0.50            \\ \hline \hline
\multicolumn{2}{|c|}{\shortstack[l]{Shuffled \\ VUA \\ + \\ TOEFL}}  &  \multicolumn{3}{l}{}   \\ \hline
\multicolumn{2}{|l|}{}                       & Pr & 0.51 (+/- 0.02) & 0.51 (+/- 0.03) \\ \cline{3-5} 
\multicolumn{2}{|l|}{}                       & Re & 0.69 (+/- 0.02) & 0.67 (+/- 0.04) \\ \cline{3-5} 
\multicolumn{2}{|l|}{}                       & F1 & 0.59            & 0.58            \\ \hline
\end{tabular}
\caption{\label{tab:sequence_learning}10-fold CV comparison of training on (un)shuffled VUA and TOEFL data for the Verb and AllPOS Task.}
\end{table}

% vuamc-toefl_on_vuamc-verb.eval            accuracy                                  0.97 (+/- 0.01)
% vuamc-toefl_on_vuamc-verb.eval            precision                                 0.48 (+/- 0.10)
% vuamc-toefl_on_vuamc-verb.eval            recall                                    0.49 (+/- 0.15)

% vuamc-toefl_on_toefl-verb.eval            accuracy                                  0.97 (+/- 0.01)
% vuamc-toefl_on_toefl-verb.eval            precision                                 0.48 (+/- 0.08)
% vuamc-toefl_on_toefl-verb.eval            recall                                    0.49 (+/- 0.14)

% vuamc-toefl_on_vuamc.eval                 accuracy                                  0.95 (+/- 0.02)
% vuamc-toefl_on_vuamc.eval                 precision                                 0.47 (+/- 0.10)
% vuamc-toefl_on_vuamc.eval                 recall                                    0.41 (+/- 0.10)

% vuamc-toefl_on_toefl.eval                 accuracy                                  0.95 (+/- 0.02)
% vuamc-toefl_on_toefl.eval                 precision                                 0.47 (+/- 0.11)
% vuamc-toefl_on_toefl.eval                 recall                                    0.41 (+/- 0.11)


% toefl-vuamc_on_vuamc-verb.eval            accuracy                                  0.96 (+/- 0.00)
% toefl-vuamc_on_vuamc-verb.eval            precision                                 0.39 (+/- 0.15)
% toefl-vuamc_on_vuamc-verb.eval            recall                                    0.79 (+/- 0.04)

% toefl-vuamc_on_toefl-verb.eval            accuracy                                  0.96 (+/- 0.00)
% toefl-vuamc_on_toefl-verb.eval            precision                                 0.40 (+/- 0.15)
% toefl-vuamc_on_toefl-verb.eval            recall                                    0.77 (+/- 0.05)

% toefl-vuamc_on_vuamc.eval                 accuracy                                  0.91 (+/- 0.01)
% toefl-vuamc_on_vuamc.eval                 precision                                 0.32 (+/- 0.11)
% toefl-vuamc_on_vuamc.eval                 recall                                    0.71 (+/- 0.24)

% toefl-vuamc_on_toefl.eval                 accuracy                                  0.91 (+/- 0.02)
% toefl-vuamc_on_toefl.eval                 precision                                 0.39 (+/- 0.23)
% toefl-vuamc_on_toefl.eval                 recall                                    0.71 (+/- 0.24)


% toefl-vuamc_on_vuamc-verb-shuf.eval       accuracy                                  0.97 (+/- 0.00)
% toefl-vuamc_on_vuamc-verb-shuf.eval       precision                                 0.51 (+/- 0.02)
% toefl-vuamc_on_vuamc-verb-shuf.eval       recall                                    0.69 (+/- 0.02)

% toefl-vuamc_on_toefl-verb-shuf.eval       accuracy                                  0.98 (+/- 0.00)
% toefl-vuamc_on_toefl-verb-shuf.eval       precision                                 0.53 (+/- 0.03)
% toefl-vuamc_on_toefl-verb-shuf.eval       recall                                    0.67 (+/- 0.06)

% toefl-vuamc_on_vuamc-shuf.eval            accuracy                                  0.95 (+/- 0.00)
% toefl-vuamc_on_vuamc-shuf.eval            precision                                 0.52 (+/- 0.04)
% toefl-vuamc_on_vuamc-shuf.eval            recall                                    0.66 (+/- 0.06)

% toefl-vuamc_on_toefl-shuf.eval            accuracy                                  0.95 (+/- 0.00)
% toefl-vuamc_on_toefl-shuf.eval            precision                                 0.51 (+/- 0.03)
% toefl-vuamc_on_toefl-shuf.eval            recall                                    0.67 (+/- 0.04)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Metadata}
In this experiment, we added available metadata as additional information to learn from. The difference is that compared to other information, such as POS tags, this metadata applies to whole sentences and the entire texts. Also, this experiment only addresses the TOEFL dataset. 

The available metadata are the following:
\begin{itemize}
\item Prompt: The prompt that triggered the production of the respective sentence and text (P1-P8)
\item Proficiency: The proficiency of the language learner who produced the text (medium or high)
\item L1: The language learner's L1 (ARA, JPN, ITA)
\item Text ID: The text's unique ID (which represents the individual language learner)
\item Text length: The length (in tokens) of the complete text the respective sentence belongs to
\end{itemize}

Given the discussion in Section 2, we expected confounding effects for some of the metadata, and we hoped to improve the results when factoring in the metadata that showed significant effects on the number and use of metaphors.

As word embeddings, we used only the BNC.
The input data was constructed for both tasks (AllPOS and Verbs) by adding the metadata information for every single word in the input sequence. 
Testing was done by 10-fold cross-validation.

Table \ref{tab:metadata} shows that, most notably, the overall metadata does \emph{not} improve the results in a systematic, meaningful way.
Also, some metadata, like the \emph{Text ID} even considerably degrades performance. Overall, there is no clear tendency towards metadata being more -- if at all - helpful.  

In addition, we evaluated some combinations on the organizers' held-out test (via CodaLab) and found that the Verb task - contrary to our 10-fold CV - gained slightly from the use of metadata. We hope to be able to evaluate this experiment on the held-out test set from the organizers. Alternatively, we could split the available dataset and re-run the experiments with this data.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\setlength{\tabcolsep}{2.5pt}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
                                                                                   &    & Verbs           & AllPOS          \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Baseline\\ (no metadt.)\end{tabular}}  & Pr & 0.53 (+/- 0.05) & 0.53 (+/- 0.04) \\ \cline{2-4} 
                                                                                   & Re & 0.64 (+/- 0.07) & 0.63 (+/- 0.05) \\ \hline
\multirow{2}{*}{Prompt}                                                            & Pr & 0.50 (+/- 0.06) & 0.51 (+/- 0.05) \\ \cline{2-4} 
                                                                                   & Re & 0.66 (+/- 0.10) & 0.64 (+/- 0.05) \\ \hline
\multirow{2}{*}{Proficiency}                                                       & Pr & 0.51 (+/- 0.07) & 0.51 (+/- 0.05) \\ \cline{2-4} 
                                                                                   & Re & 0.68 (+/- 0.09) & 0.65 (+/- 0.07) \\ \hline
\multirow{2}{*}{L1}                                                                & Pr & 0.49 (+/- 0.05) & 0.53 (+/- 0.06) \\ \cline{2-4} 
                                                                                   & Re & 0.68 (+/- 0.08) & 0.60 (+/- 0.06) \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Prom.+  \\ Prof. + L1\end{tabular}} & Pr & 0.54 (+/- 0.07) & 0.52 (+/- 0.06) \\ \cline{2-4} 
                                                                                   & Re & 0.62 (+/- 0.07) & 0.64 (+/- 0.06) \\ \hline
\multirow{2}{*}{Text ID}                                                           & Pr & 0.42 (+/- 0.08) & 0.48 (+/- 0.08) \\ \cline{2-4} 
                                                                                   & Re & 0.72 (+/- 0.09) & 0.60 (+/- 0.10) \\ \hline
\multirow{2}{*}{Text length}                                                       & Pr & 0.50 (+/- 0.06) & 0.53 (+/- 0.06) \\ \cline{2-4} 
                                                                                   & Re & 0.66 (+/- 0.11) & 0.62 (+/- 0.05) \\ \hline
\multirow{2}{*}{All}                                                               & Pr & 0.45 (+/- 0.08) & 0.44 (+/- 0.08) \\ \cline{2-4} 
                                                                                   & Re & 0.68 (+/- 0.08) & 0.64 (+/- 0.08) \\ \hline
\end{tabular}
\end{center}
\caption{\label{tab:metadata}10-fold CV comparison of training on different metadata.}
\end{table}

% toefl-all_pos-bnc.eval                    accuracy                                  0.97 (+/- 0.00)
% toefl-all_pos-bnc.eval                    precision                                 0.53 (+/- 0.04)
% toefl-all_pos-bnc.eval                    recall                                    0.63 (+/- 0.05)

% toefl-all_pos-bnc-meta_cat_prom.eval      accuracy                                  0.97 (+/- 0.00)
% toefl-all_pos-bnc-meta_cat_prom.eval      precision                                 0.51 (+/- 0.05)
% toefl-all_pos-bnc-meta_cat_prom.eval      recall                                    0.64 (+/- 0.05)

% toefl-all_pos-bnc-meta_cat_prof.eval      accuracy                                  0.97 (+/- 0.00)
% toefl-all_pos-bnc-meta_cat_prof.eval      precision                                 0.51 (+/- 0.05)
% toefl-all_pos-bnc-meta_cat_prof.eval      recall                                    0.65 (+/- 0.07)

% toefl-all_pos-bnc-meta_cat_l1.eval        accuracy                                  0.97 (+/- 0.00)
% toefl-all_pos-bnc-meta_cat_l1.eval        precision                                 0.53 (+/- 0.06)
% toefl-all_pos-bnc-meta_cat_l1.eval        recall                                    0.60 (+/- 0.06)

% toefl-all_pos-bnc-meta_cats.eval          accuracy                                  0.97 (+/- 0.00)
% toefl-all_pos-bnc-meta_cats.eval          precision                                 0.52 (+/- 0.06)
% toefl-all_pos-bnc-meta_cats.eval          recall                                    0.64 (+/- 0.06)

% toefl-all_pos-bnc-meta_ids.eval           accuracy                                  0.97 (+/- 0.01)
% toefl-all_pos-bnc-meta_ids.eval           precision                                 0.48 (+/- 0.08)
% toefl-all_pos-bnc-meta_ids.eval           recall                                    0.60 (+/- 0.10)

% toefl-all_pos-bnc-meta_len.eval           accuracy                                  0.97 (+/- 0.00)
% toefl-all_pos-bnc-meta_len.eval           precision                                 0.53 (+/- 0.06)
% toefl-all_pos-bnc-meta_len.eval           recall                                    0.62 (+/- 0.05)

% toefl-all_pos-bnc-meta_all.eval           accuracy                                  0.96 (+/- 0.01)
% toefl-all_pos-bnc-meta_all.eval           precision                                 0.44 (+/- 0.08)
% toefl-all_pos-bnc-meta_all.eval           recall                                    0.64 (+/- 0.08)


% toefl-verb-bnc.eval                       accuracy                                  0.99 (+/- 0.00)
% toefl-verb-bnc.eval                       precision                                 0.53 (+/- 0.05)
% toefl-verb-bnc.eval                       recall                                    0.64 (+/- 0.07)

% toefl-verb-bnc-meta_cat_prom.eval         accuracy                                  0.98 (+/- 0.00)
% toefl-verb-bnc-meta_cat_prom.eval         precision                                 0.50 (+/- 0.06)
% toefl-verb-bnc-meta_cat_prom.eval         recall                                    0.66 (+/- 0.10)

% toefl-verb-bnc-meta_cat_prof.eval         accuracy                                  0.99 (+/- 0.00)
% toefl-verb-bnc-meta_cat_prof.eval         precision                                 0.51 (+/- 0.07)
% toefl-verb-bnc-meta_cat_prof.eval         recall                                    0.68 (+/- 0.09)

% toefl-verb-bnc-meta_cat_l1.eval           accuracy                                  0.98 (+/- 0.00)
% toefl-verb-bnc-meta_cat_l1.eval           precision                                 0.49 (+/- 0.05)
% toefl-verb-bnc-meta_cat_l1.eval           recall                                    0.68 (+/- 0.08)

% toefl-verb-bnc-meta_cats.eval             accuracy                                  0.99 (+/- 0.00)
% toefl-verb-bnc-meta_cats.eval             precision                                 0.54 (+/- 0.07)
% toefl-verb-bnc-meta_cats.eval             recall                                    0.62 (+/- 0.07)

% toefl-verb-bnc-meta_ids.eval              accuracy                                  0.98 (+/- 0.00)
% toefl-verb-bnc-meta_ids.eval              precision                                 0.42 (+/- 0.08)
% toefl-verb-bnc-meta_ids.eval              recall                                    0.72 (+/- 0.09)

% toefl-verb-bnc-meta_len.eval              accuracy                                  0.98 (+/- 0.00)
% toefl-verb-bnc-meta_len.eval              precision                                 0.50 (+/- 0.06)
% toefl-verb-bnc-meta_len.eval              recall                                    0.66 (+/- 0.11)

% toefl-verb-bnc-meta_all.eval              accuracy                                  0.98 (+/- 0.01)
% toefl-verb-bnc-meta_all.eval              precision                                 0.45 (+/- 0.08)
% toefl-verb-bnc-meta_all.eval              recall                                    0.68 (+/- 0.08)



%- Vergleich: BNC vs BNC + NLI

\section{Conclusion}

This paper has focused on the structure of the learner data used in the Second Shared Task of Metaphor Identification. We aimed at exploring possible factors that influence this kind of data and tested whether these play a role for the automated identification using word-embeddings in an LSTM BiRNN architecture. A descriptive investigation of the manually annotated sample of the ETS Corpus of Non-Native Written English (TOEFL11 corpus) shows that the factors of proficiency and especially the essay prompt exhibit significant correlations to the amount and type of metaphors found in the annotated training set. The data also shows a numerical bias towards the annotation of verbs as metaphors compared to other content words.

 A sequential training of the bidirectional neural network using both the VUA and the TOEFL partitions of the shared task points to the different structure of the datasets, in particular towards an emerging bias of overidentifying verbal metaphors in the neural network based classification. The hypothesized influence of the metadata in the TOEFL set, in particular the observed dependencies on proficiency and the essay prompt, was not confirmed by the results of the automated identification. While the factors of L1, proficiency, prompt and essay length did not influence the baseline results, the essay ID (i.e. the individual learner) reduced the performance of the system as did a combination of all metadata. For the future, more tests with different ways of modelling the metadata in the neural network architecture and on the test set of the task will provide further insights.


\section*{Acknowledgments}
The computational results presented have been achieved in part using the \href{http://vsc.ac.at}{Vienna Scientific Cluster (VSC)}.

%\section*{References}
%\leftskip 0.15in%
%\parindent -0.15in%
%\setlength{\parskip}{7pt}% 
%\noindent
%
%
\bibliography{anthology,acl2020,egon}
\bibliographystyle{acl_natbib}
\end{document}